[2023-01-28 00:57:44,600] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.local_to_gcs_task scheduled__2019-01-01T00:00:00+00:00 [queued]>
[2023-01-28 00:57:44,611] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.local_to_gcs_task scheduled__2019-01-01T00:00:00+00:00 [queued]>
[2023-01-28 00:57:44,612] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2023-01-28 00:57:44,612] {taskinstance.py:1239} INFO - Starting attempt 1 of 2
[2023-01-28 00:57:44,613] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2023-01-28 00:57:44,624] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2019-01-01 00:00:00+00:00
[2023-01-28 00:57:44,629] {standard_task_runner.py:52} INFO - Started process 929 to run task
[2023-01-28 00:57:44,633] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'local_to_gcs_task', 'scheduled__2019-01-01T00:00:00+00:00', '--job-id', '74', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpaw6gnzbg', '--error-file', '/tmp/tmpe714mgb7']
[2023-01-28 00:57:44,635] {standard_task_runner.py:77} INFO - Job 74: Subtask local_to_gcs_task
[2023-01-28 00:57:44,698] {logging_mixin.py:109} INFO - Running <TaskInstance: data_ingestion_gcs_dag.local_to_gcs_task scheduled__2019-01-01T00:00:00+00:00 [running]> on host adbf820de9e6
[2023-01-28 00:57:44,740] {logging_mixin.py:109} WARNING - /home/***/.local/lib/python3.7/site-packages/***/utils/context.py:152 AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
[2023-01-28 00:57:44,763] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=data_ingestion_gcs_dag
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2019-01-01T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2019-01-01T00:00:00+00:00
[2023-01-28 01:00:32,900] {python.py:175} INFO - Done. Returned value was: None
[2023-01-28 01:00:32,964] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=data_ingestion_gcs_dag, task_id=local_to_gcs_task, execution_date=20190101T000000, start_date=20230128T005744, end_date=20230128T010032
[2023-01-28 01:00:33,004] {local_task_job.py:154} INFO - Task exited with return code 0
[2023-01-28 01:00:33,051] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-01-29 12:51:14,072] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.local_to_gcs_task scheduled__2019-01-01T00:00:00+00:00 [queued]>
[2023-01-29 12:51:14,084] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.local_to_gcs_task scheduled__2019-01-01T00:00:00+00:00 [queued]>
[2023-01-29 12:51:14,085] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2023-01-29 12:51:14,086] {taskinstance.py:1239} INFO - Starting attempt 1 of 2
[2023-01-29 12:51:14,087] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2023-01-29 12:51:14,097] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2019-01-01 00:00:00+00:00
[2023-01-29 12:51:14,103] {standard_task_runner.py:52} INFO - Started process 257 to run task
[2023-01-29 12:51:14,107] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'local_to_gcs_task', 'scheduled__2019-01-01T00:00:00+00:00', '--job-id', '434', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmptrdtxlry', '--error-file', '/tmp/tmpdvpcx8za']
[2023-01-29 12:51:14,109] {standard_task_runner.py:77} INFO - Job 434: Subtask local_to_gcs_task
[2023-01-29 12:51:14,165] {logging_mixin.py:109} INFO - Running <TaskInstance: data_ingestion_gcs_dag.local_to_gcs_task scheduled__2019-01-01T00:00:00+00:00 [running]> on host 2530f37057aa
[2023-01-29 12:51:14,218] {logging_mixin.py:109} WARNING - /home/***/.local/lib/python3.7/site-packages/***/utils/context.py:152 AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
[2023-01-29 12:51:14,252] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=data_ingestion_gcs_dag
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2019-01-01T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2019-01-01T00:00:00+00:00
[2023-01-29 12:51:42,543] {taskinstance.py:1700} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 58, in upload_to_gcs
    blob.upload_from_filename(local_file)
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py", line 2735, in upload_from_filename
    retry=retry,
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py", line 2594, in upload_from_file
    retry=retry,
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py", line 2412, in _do_upload
    retry=retry,
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py", line 2242, in _do_resumable_upload
    response = upload.transmit_next_chunk(transport, timeout=timeout)
  File "/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/requests/upload.py", line 503, in transmit_next_chunk
    method, url, payload, headers = self._prepare_request()
  File "/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/_upload.py", line 605, in _prepare_request
    self._stream, self._chunk_size, self._total_bytes
  File "/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/_upload.py", line 982, in get_next_chunk
    "Stream is already exhausted. There is no content remaining."
ValueError: Stream is already exhausted. There is no content remaining.
[2023-01-29 12:51:42,589] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=data_ingestion_gcs_dag, task_id=local_to_gcs_task, execution_date=20190101T000000, start_date=20230129T125114, end_date=20230129T125142
[2023-01-29 12:51:42,643] {standard_task_runner.py:92} ERROR - Failed to execute job 434 for task local_to_gcs_task
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 298, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 107, in _run_task_by_selected_method
    _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 184, in _run_raw_task
    error_file=args.error_file,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 58, in upload_to_gcs
    blob.upload_from_filename(local_file)
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py", line 2735, in upload_from_filename
    retry=retry,
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py", line 2594, in upload_from_file
    retry=retry,
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py", line 2412, in _do_upload
    retry=retry,
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py", line 2242, in _do_resumable_upload
    response = upload.transmit_next_chunk(transport, timeout=timeout)
  File "/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/requests/upload.py", line 503, in transmit_next_chunk
    method, url, payload, headers = self._prepare_request()
  File "/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/_upload.py", line 605, in _prepare_request
    self._stream, self._chunk_size, self._total_bytes
  File "/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/_upload.py", line 982, in get_next_chunk
    "Stream is already exhausted. There is no content remaining."
ValueError: Stream is already exhausted. There is no content remaining.
[2023-01-29 12:51:42,691] {local_task_job.py:154} INFO - Task exited with return code 1
[2023-01-29 12:51:42,769] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-29 12:57:39,513] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.local_to_gcs_task scheduled__2019-01-01T00:00:00+00:00 [queued]>
[2023-01-29 12:57:39,738] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.local_to_gcs_task scheduled__2019-01-01T00:00:00+00:00 [queued]>
[2023-01-29 12:57:39,740] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2023-01-29 12:57:39,741] {taskinstance.py:1239} INFO - Starting attempt 1 of 2
[2023-01-29 12:57:39,743] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2023-01-29 12:57:39,766] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2019-01-01 00:00:00+00:00
[2023-01-29 12:57:39,810] {standard_task_runner.py:52} INFO - Started process 1291 to run task
[2023-01-29 12:57:40,084] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'local_to_gcs_task', 'scheduled__2019-01-01T00:00:00+00:00', '--job-id', '441', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpcdrwp_jy', '--error-file', '/tmp/tmp47l5dlnw']
[2023-01-29 12:57:40,105] {standard_task_runner.py:77} INFO - Job 441: Subtask local_to_gcs_task
[2023-01-29 12:57:46,412] {logging_mixin.py:109} INFO - Running <TaskInstance: data_ingestion_gcs_dag.local_to_gcs_task scheduled__2019-01-01T00:00:00+00:00 [running]> on host 2530f37057aa
[2023-01-29 12:57:46,750] {logging_mixin.py:109} WARNING - /home/***/.local/lib/python3.7/site-packages/***/utils/context.py:152 AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
[2023-01-29 12:57:46,845] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=data_ingestion_gcs_dag
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2019-01-01T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2019-01-01T00:00:00+00:00
[2023-01-29 12:57:48,325] {taskinstance.py:1700} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py", line 2594, in upload_from_file
    retry=retry,
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py", line 2412, in _do_upload
    retry=retry,
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py", line 2242, in _do_resumable_upload
    response = upload.transmit_next_chunk(transport, timeout=timeout)
  File "/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/requests/upload.py", line 516, in transmit_next_chunk
    retriable_request, self._get_status_code, self._retry_strategy
  File "/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/requests/_request_helpers.py", line 147, in wait_and_retry
    response = func()
  File "/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/requests/upload.py", line 511, in retriable_request
    self._process_response(result, len(payload))
  File "/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/_upload.py", line 675, in _process_response
    callback=self._make_invalid,
  File "/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/_helpers.py", line 104, in require_status_code
    *status_codes
google.resumable_media.common.InvalidResponse: ('Request failed with status code', 400, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PERMANENT_REDIRECT: 308>)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 58, in upload_to_gcs
    blob.upload_from_filename(local_file)
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py", line 2735, in upload_from_filename
    retry=retry,
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py", line 2598, in upload_from_file
    _raise_from_invalid_response(exc)
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py", line 4466, in _raise_from_invalid_response
    raise exceptions.from_http_status(response.status_code, message, response=response)
google.api_core.exceptions.BadRequest: 400 PUT https://storage.googleapis.com/upload/storage/v1/b/dtc_data_lake_ny-rides-kelvin/o?uploadType=resumable&upload_id=ADPycdt6scQ5xqhwbGWdc7w5T1JyLHZnv2vXEDHhz5Yc0lM8DzRXfLQlmiqFjV8htJXQEENQ3xM4TVaXpAKBDNy6McMTog: Invalid request.  The number of bytes uploaded is required to be equal or greater than 262144, except for the final request (it's recommended to be the exact multiple of 262144).  The received request contained 28672 bytes, which does not meet this requirement.: ('Request failed with status code', 400, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PERMANENT_REDIRECT: 308>)
[2023-01-29 12:57:48,357] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=data_ingestion_gcs_dag, task_id=local_to_gcs_task, execution_date=20190101T000000, start_date=20230129T125739, end_date=20230129T125748
[2023-01-29 12:57:48,388] {standard_task_runner.py:92} ERROR - Failed to execute job 441 for task local_to_gcs_task
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py", line 2594, in upload_from_file
    retry=retry,
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py", line 2412, in _do_upload
    retry=retry,
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py", line 2242, in _do_resumable_upload
    response = upload.transmit_next_chunk(transport, timeout=timeout)
  File "/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/requests/upload.py", line 516, in transmit_next_chunk
    retriable_request, self._get_status_code, self._retry_strategy
  File "/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/requests/_request_helpers.py", line 147, in wait_and_retry
    response = func()
  File "/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/requests/upload.py", line 511, in retriable_request
    self._process_response(result, len(payload))
  File "/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/_upload.py", line 675, in _process_response
    callback=self._make_invalid,
  File "/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/_helpers.py", line 104, in require_status_code
    *status_codes
google.resumable_media.common.InvalidResponse: ('Request failed with status code', 400, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PERMANENT_REDIRECT: 308>)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 298, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 107, in _run_task_by_selected_method
    _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 184, in _run_raw_task
    error_file=args.error_file,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 58, in upload_to_gcs
    blob.upload_from_filename(local_file)
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py", line 2735, in upload_from_filename
    retry=retry,
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py", line 2598, in upload_from_file
    _raise_from_invalid_response(exc)
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py", line 4466, in _raise_from_invalid_response
    raise exceptions.from_http_status(response.status_code, message, response=response)
google.api_core.exceptions.BadRequest: 400 PUT https://storage.googleapis.com/upload/storage/v1/b/dtc_data_lake_ny-rides-kelvin/o?uploadType=resumable&upload_id=ADPycdt6scQ5xqhwbGWdc7w5T1JyLHZnv2vXEDHhz5Yc0lM8DzRXfLQlmiqFjV8htJXQEENQ3xM4TVaXpAKBDNy6McMTog: Invalid request.  The number of bytes uploaded is required to be equal or greater than 262144, except for the final request (it's recommended to be the exact multiple of 262144).  The received request contained 28672 bytes, which does not meet this requirement.: ('Request failed with status code', 400, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PERMANENT_REDIRECT: 308>)
[2023-01-29 12:57:48,450] {local_task_job.py:154} INFO - Task exited with return code 1
[2023-01-29 12:57:48,634] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
